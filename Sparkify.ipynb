{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Sparkify Project') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "path = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and the datatypes stored in each\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the actual data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "|           Downgrade|\n",
      "|               Error|\n",
      "|                Help|\n",
      "|                Home|\n",
      "|               Login|\n",
      "|              Logout|\n",
      "|            NextSong|\n",
      "|            Register|\n",
      "|         Roll Advert|\n",
      "|       Save Settings|\n",
      "|            Settings|\n",
      "|    Submit Downgrade|\n",
      "| Submit Registration|\n",
      "|      Submit Upgrade|\n",
      "|         Thumbs Down|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See all of the possible pages\n",
    "df.select('page').dropDuplicates().sort('page').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove Missing User ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the unqiue user ID's\n",
    "df.select('userId').dropDuplicates().sort('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "|       10|\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       15|\n",
      "|       16|\n",
      "|       17|\n",
      "|       18|\n",
      "|       19|\n",
      "|       20|\n",
      "|       21|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the unique Session ID's\n",
    "df.select('sessionId').dropDuplicates().sort('sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank User ID's\n",
    "# Session ID's did not need to be filtered because none were missing\n",
    "df = df.filter(df[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "|100018|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the blank user ID is no longer in the dataset\n",
    "df.select('userId').dropDuplicates().sort('userId').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed ts dtype so it can be more easily read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the timestamp column to a more readable time column\n",
    "get_time = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "df = df.withColumn(\"time\", get_time(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', time='2018-09-30 20:01:57')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the formating of the time column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag every churn that occurs with a 1\n",
    "flag_cancellation_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag every downgrade that occurs with a 1\n",
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Downgrade\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add churn flag to dataframe\n",
    "df = df.withColumn('flag_cancellation', flag_cancellation_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add downgrade flag to dataframe\n",
    "df = df.withColumn('flag_downgrade', flag_downgrade_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', time='2018-09-30 20:01:57', flag_cancellation=0, flag_downgrade=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if it got implemented properly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create window function\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(F.desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create churn column which checks to see if user cancelled in the past\n",
    "df = df.withColumn(\"churn\", Fsum(\"flag_cancellation\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dwongrade column which checks to see if user downgraded in the past\n",
    "df = df.withColumn(\"downgrade\", Fsum(\"flag_downgrade\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist=None, auth='Logged In', firstName='Darianna', gender='F', itemInSession=34, lastName='Carpenter', length=None, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='Logout', registration=1538016340000, sessionId=187, song=None, status=307, ts=1542823952000, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', time='2018-11-21 13:12:32', flag_cancellation=0, flag_downgrade=0, churn=0, downgrade=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure it was implemented properly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|flag_cancellation| count|\n",
      "+-----------------+------+\n",
      "|                1|    52|\n",
      "|                0|278102|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the amount of users that churned\n",
    "df.groupby('flag_cancellation').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|flag_downgrade| count|\n",
      "+--------------+------+\n",
      "|             1|  2055|\n",
      "|             0|276099|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the amount of users that downgraded\n",
    "df.groupby('flag_downgrade').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|churn| count|\n",
      "+-----+------+\n",
      "|    0|233290|\n",
      "|    1| 44864|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an idea for churn amounts\n",
    "df.groupby('churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|154578|\n",
      "|     M|123576|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an idea for the distributions of gender\n",
    "df.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|level| count|\n",
      "+-----+------+\n",
      "| free| 55721|\n",
      "| paid|222433|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an idea for the distributions of the types of service each customer recieves\n",
    "df.groupby('level').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "      <th>churn</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "      <td>24429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "      <td>18904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "      <td>1</td>\n",
       "      <td>5243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "      <td>1</td>\n",
       "      <td>7145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>paid</td>\n",
       "      <td>0</td>\n",
       "      <td>110981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>paid</td>\n",
       "      <td>0</td>\n",
       "      <td>78976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "      <td>paid</td>\n",
       "      <td>1</td>\n",
       "      <td>13925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>paid</td>\n",
       "      <td>1</td>\n",
       "      <td>18551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender level  churn   count\n",
       "0      F  free      0   24429\n",
       "1      M  free      0   18904\n",
       "2      F  free      1    5243\n",
       "3      M  free      1    7145\n",
       "4      F  paid      0  110981\n",
       "5      M  paid      0   78976\n",
       "6      F  paid      1   13925\n",
       "7      M  paid      1   18551"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break down churned users by level of service and gender\n",
    "df_pd = df.groupby(['gender', 'level', 'churn']).count().toPandas()\n",
    "df_pd = df_pd.sort_values(['level', 'churn', 'gender'], ignore_index = True)\n",
    "\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a visual of the above data so the distributions can be more easily seen\n",
    "df_pd.plot(kind='bar', legend = None, y = 'count')\n",
    "plt.title('Amount of Churns')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create level dummy variables\n",
    "get_paid = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "get_free = udf(lambda x: 1 if x == 'free' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables to dataframe\n",
    "df = df.withColumn('paid', get_paid(df.level))\n",
    "df = df.withColumn('free', get_free(df.level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gender dummy variables\n",
    "get_male = udf(lambda x: 1 if x == 'M' else 0, IntegerType())\n",
    "get_female = udf(lambda x: 1 if x == 'F' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables to dataframe\n",
    "df = df.withColumn('male', get_male(df.gender))\n",
    "df = df.withColumn('female', get_female(df.gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist=None, auth='Logged In', firstName='Darianna', gender='F', itemInSession=34, lastName='Carpenter', length=None, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='Logout', registration=1538016340000, sessionId=187, song=None, status=307, ts=1542823952000, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', time='2018-11-21 13:12:32', flag_cancellation=0, flag_downgrade=0, churn=0, downgrade=0, paid=0, free=1, male=0, female=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dummy variables\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn columns into Vectors\n",
    "assembler = VectorAssembler(inputCols=[\"churn\"], outputCol=\"ChurnVec\")\n",
    "assembler2 = VectorAssembler(inputCols=[\"downgrade\"], outputCol=\"DownVec\")\n",
    "assembler3 = VectorAssembler(inputCols=[\"paid\", \"free\"], outputCol=\"LevelVec\")\n",
    "assembler4 = VectorAssembler(inputCols=[\"male\", \"female\"], outputCol=\"GenderVec\")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "df = assembler2.transform(df)\n",
    "df = assembler3.transform(df)\n",
    "df = assembler4.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist=None, auth='Logged In', firstName='Darianna', gender='F', itemInSession=34, lastName='Carpenter', length=None, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='Logout', registration=1538016340000, sessionId=187, song=None, status=307, ts=1542823952000, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', time='2018-11-21 13:12:32', flag_cancellation=0, flag_downgrade=0, churn=0, downgrade=0, paid=0, free=1, male=0, female=1, ChurnVec=DenseVector([0.0]), DownVec=DenseVector([0.0]), LevelVec=DenseVector([0.0, 1.0]), GenderVec=DenseVector([0.0, 1.0]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check new vectors\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the vectors\n",
    "scaler_q1 = Normalizer(inputCol=\"ChurnVec\", outputCol=\"ChurnVecNormalizer\")\n",
    "scaler_q2 = Normalizer(inputCol=\"DownVec\", outputCol=\"DownVecNormalizer\")\n",
    "scaler_q3 = Normalizer(inputCol=\"LevelVec\", outputCol=\"LevelVecNormalizer\")\n",
    "scaler_q4 = Normalizer(inputCol=\"GenderVec\", outputCol=\"GenderVecNormalizer\")\n",
    "\n",
    "df = scaler_q1.transform(df)\n",
    "df = scaler_q2.transform(df)\n",
    "df = scaler_q3.transform(df)\n",
    "df = scaler_q4.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist=None, auth='Logged In', firstName='Darianna', gender='F', itemInSession=34, lastName='Carpenter', length=None, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='Logout', registration=1538016340000, sessionId=187, song=None, status=307, ts=1542823952000, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', time='2018-11-21 13:12:32', flag_cancellation=0, flag_downgrade=0, churn=0, downgrade=0, paid=0, free=1, male=0, female=1, ChurnVec=DenseVector([0.0]), DownVec=DenseVector([0.0]), LevelVec=DenseVector([0.0, 1.0]), GenderVec=DenseVector([0.0, 1.0]), ChurnVecNormalizer=DenseVector([0.0]), DownVecNormalizer=DenseVector([0.0]), LevelVecNormalizer=DenseVector([0.0, 1.0]), GenderVecNormalizer=DenseVector([0.0, 1.0]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the normalization\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalized data into a features column\n",
    "assembler5 = VectorAssembler(inputCols=[\"ChurnVecNormalizer\", \"DownVecNormalizer\", \"LevelVecNormalizer\", \\\n",
    "                                       \"GenderVecNormalizer\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist=None, auth='Logged In', firstName='Darianna', gender='F', itemInSession=34, lastName='Carpenter', length=None, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='Logout', registration=1538016340000, sessionId=187, song=None, status=307, ts=1542823952000, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', time='2018-11-21 13:12:32', flag_cancellation=0, flag_downgrade=0, churn=0, downgrade=0, paid=0, free=1, male=0, female=1, ChurnVec=DenseVector([0.0]), DownVec=DenseVector([0.0]), LevelVec=DenseVector([0.0, 1.0]), GenderVec=DenseVector([0.0, 1.0]), ChurnVecNormalizer=DenseVector([0.0]), DownVecNormalizer=DenseVector([0.0]), LevelVecNormalizer=DenseVector([0.0, 1.0]), GenderVecNormalizer=DenseVector([0.0, 1.0]), features=SparseVector(6, {3: 1.0, 5: 1.0}))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the features column\n",
    "df = assembler5.transform(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0, features=SparseVector(6, {3: 1.0, 5: 1.0}))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subset of the datafarme which will be used for modeling\n",
    "data = df.select(F.col(\"churn\").alias(\"label\"), F.col('features'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree\n",
    "clf = DecisionTreeClassifier(maxDepth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator\n",
    "evaluator= MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing data\n",
    "train, test = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Cross Validation optimizer\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(clf.maxDepth, [2, 10]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=clf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get starting time to train the model\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and optimize the given parameters\n",
    "cvModel = clf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453.9976315498352\n"
     ]
    }
   ],
   "source": [
    "# Calculate training time\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='DecisionTreeClassifier_ea7ceaa5c5c8', name='seed', doc='random seed.'): 5421475168487575420}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the parameters of the tuned model\n",
    "cvModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predictions\n",
    "pred = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F-1 Score:1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the accuracy and F1 Score\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(pred.select('label','prediction'), {evaluator.metricName: \"accuracy\"})))\n",
    "print('F-1 Score:{}'.format(evaluator.evaluate(pred.select('label','prediction'), {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These results show that my model is very accurate at predicting if a user will churn. This means that our model was efficiently tuned for efficient predictions. However there is some things that can be improved. One improvement could be storing the amount of time the user has had each level of service. For example, someone who had the paid service for longer might be less likely to churn compared to someone who just got it. This extra information might be able to make our model even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
